# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18a3V87H3HYUpDBDDVGLk4YiSwlFwfHdG

# Capstone Proyek : [Prediksi Penyakit Tanaman Berbasis Citra]

## Import Semua Packages/Library yang Digunakan
"""

# Import library yang diperlukan
import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.applications import MobileNetV2
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import zipfile
from google.colab import files
from google.colab import drive
import cv2
import shutil, random

# Cek versi
print("TensorFlow version:", tf.__version__)
print("Pandas version:", pd.__version__)
print("Python version:", sys.version)

# Cek GPU
print("GPU available:", tf.config.list_physical_devices('GPU'))

# Mengabaikan peringatan
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## Data Preparation

### Data Loading

### 1. Mount Google Drive
"""

drive.mount('/content/drive')

"""### 2. Ekstraksi ZIP Dataset"""

zip_path = "/content/drive/MyDrive/Model CP/4 dataset citra daun tanaman.zip"
extract_path = "dataset"

if not os.path.exists(extract_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    print("Dataset berhasil diekstrak.")
else:
    print("Folder sudah ada, tidak perlu ekstrak ulang.")

"""### 3. Cek isi folder"""

path_kelas = "/content/dataset/4 dataset citra daun tanaman"
print("Daftar kelas dalam dataset:")
print(os.listdir(path_kelas))

"""### 4. Set seed"""

np.random.seed(42)
tf.random.set_seed(42)

"""### Data Preprocessing

#### Split Dataset

### 5. Split Dataset ke Train / Validation / Test
"""

original_dataset_dir = os.path.join(extract_path, '4 dataset citra daun tanaman')
split_base_dir = os.path.join(extract_path, 'dataset_split')
train_dir = os.path.join(split_base_dir, 'train')
valid_dir = os.path.join(split_base_dir, 'validation')
test_dir  = os.path.join(split_base_dir, 'test')

for dir_path in [train_dir, valid_dir, test_dir]:
    os.makedirs(dir_path, exist_ok=True)

train_ratio = 0.7
val_ratio = 0.2
test_ratio = 0.1

for class_name in os.listdir(original_dataset_dir):
    class_path = os.path.join(original_dataset_dir, class_name)
    if not os.path.isdir(class_path):
        continue

    images = os.listdir(class_path)
    random.shuffle(images)

    total = len(images)
    train_len = int(train_ratio * total)
    val_len = int(val_ratio * total)

    train_images = images[:train_len]
    val_images = images[train_len:train_len + val_len]
    test_images = images[train_len + val_len:]

    def copy_images(image_list, target_root):
        class_target_dir = os.path.join(target_root, class_name)
        os.makedirs(class_target_dir, exist_ok=True)
        for img in image_list:
            src = os.path.join(class_path, img)
            dst = os.path.join(class_target_dir, img)
            shutil.copy2(src, dst)

    copy_images(train_images, train_dir)
    copy_images(val_images, valid_dir)
    copy_images(test_images, test_dir)

print("Dataset berhasil dibagi menjadi Train (70%), Validation (20%), dan Test (10%).")

"""### 6. Update path dataset"""

base_dir = split_base_dir
train_dir = os.path.join(base_dir, 'train')
valid_dir = os.path.join(base_dir, 'validation')
test_dir  = os.path.join(base_dir, 'test')

"""### 7. Cek jumlah kelas dan gambar"""

num_classes = len(os.listdir(train_dir))
train_count = sum([len(files) for r, d, files in os.walk(train_dir)])
test_count = sum([len(files) for r, d, files in os.walk(test_dir)])
valid_count = sum([len(files) for r, d, files in os.walk(valid_dir)])

print(f"\nJumlah kelas            : {num_classes}")
print(f"Jumlah gambar training  : {train_count}")
print(f"Jumlah gambar validasi  : {valid_count}")
print(f"Jumlah gambar testing   : {test_count}")

"""### 8. Pengecekan resolusi gambar contoh"""

sample_img_path = os.path.join(train_dir, os.listdir(train_dir)[0], os.listdir(os.path.join(train_dir, os.listdir(train_dir)[0]))[0])
sample_img = plt.imread(sample_img_path)
print(f"\n Ukuran gambar contoh    : {sample_img.shape}")

"""### 9. Visualisasi contoh gambar"""

plt.figure(figsize=(15, 15))
species_dirs = os.listdir(train_dir)[:9]

for i, species in enumerate(species_dirs):
    img_path = os.path.join(train_dir, species, os.listdir(os.path.join(train_dir, species))[0])
    img = plt.imread(img_path)
    plt.subplot(3, 3, i + 1)
    plt.imshow(img)
    plt.title(species)
    plt.axis('off')

plt.suptitle("Contoh Gambar Citra Daun Tanaman", fontsize=16)
plt.tight_layout()
plt.show()

# Menentukan dimensi gambar dan parameter lainnya
IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

# Data Augmentation untuk training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Hanya rescaling untuk validasi dan testing
valid_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Membuat generator
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=True
)

valid_generator = valid_datagen.flow_from_directory(
    valid_dir,  # Diubah dari train_dir ke valid_dir
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

test_generator = test_datagen.flow_from_directory(
    test_dir,  # Diubah dari train_dir ke test_dir
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

# Mendapatkan class indices dan nama kelas
class_indices = train_generator.class_indices
class_names = list(class_indices.keys())
print("Nama kelas:", class_names[:5], "...")  # Menampilkan 5 kelas pertama

# Menyimpan label untuk TFLite
with open('class_names.txt', 'w') as f:
    for class_name in class_names:
        f.write(f"{class_name}\n")

"""## Modelling"""

# Mengubah Arsitektur Model dengan Dropout yang lebih banyak dan Regularisasi L2
def create_model_modified(input_shape, num_classes):
    model = Sequential([
        # Blok Konvolusi 1
        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),
        BatchNormalization(),
        Conv2D(32, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.3), # Meningkatkan Dropout

        # Blok Konvolusi 2
        Conv2D(64, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        Conv2D(64, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.3), # Meningkatkan Dropout

        # Blok Konvolusi 3
        Conv2D(128, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        Conv2D(128, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.4), # Meningkatkan Dropout

        # Blok Konvolusi 4 (Opsional: bisa dihapus jika ingin mengurangi kompleksitas)
        Conv2D(256, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        Conv2D(256, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),
        Dropout(0.4), # Meningkatkan Dropout


        # Fully Connected Layers
        Flatten(),
        Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), # Menambahkan Regularisasi L2
        BatchNormalization(),
        Dropout(0.6), # Meningkatkan Dropout
        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)), # Menambahkan Regularisasi L2
        BatchNormalization(),
        Dropout(0.6), # Meningkatkan Dropout
        Dense(num_classes, activation='softmax')
    ])

    # Kompilasi model dengan Learning Rate yang lebih kecil (opsional)
    model.compile(
        optimizer=optimizers.Adam(learning_rate=0.0005), # Mengurangi learning rate
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# Buat model dengan arsitektur yang dimodifikasi
input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)
model = create_model_modified(input_shape, num_classes)

# Tampilkan ringkasan model
model.summary()

# Callbacks
checkpoint_path = "/content/drive/MyDrive/Model CP/models/best_model.keras"

checkpoint = ModelCheckpoint(
    checkpoint_path,
    monitor='val_accuracy',
    verbose=1,
    save_best_only=True,
    mode='max'
)

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-6,
    verbose=1
)

callbacks = [checkpoint, early_stopping, reduce_lr]

# Training model
EPOCHS = 30

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=EPOCHS,
    validation_data=valid_generator,
    validation_steps=len(valid_generator),
    callbacks=callbacks
)

"""## Evaluasi dan Visualisasi"""

# Evaluasi model pada data TEST
test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_generator), verbose=1)
print(f"\nTest Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy*100:.2f}%")

# Visualisasi Akurasi dan Loss
history_dict = history.history

plt.figure(figsize=(12, 6))

# Plot Akurasi
plt.subplot(1, 2, 1)
plt.plot(history_dict['accuracy'], label='Train Accuracy')
plt.plot(history_dict['val_accuracy'], label='Validation Accuracy')
plt.title('Akurasi Model')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history_dict['loss'], label='Train Loss')
plt.plot(history_dict['val_loss'], label='Validation Loss')
plt.title('Loss Model')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""## Konversi Model"""

keras_model_path = "/content/drive/MyDrive/Model CP/models/best_model.keras"
model = load_model(keras_model_path)

# Path tujuan
saved_model_dir = '/content/drive/MyDrive/Model CP/models/saved_model'
os.makedirs(saved_model_dir, exist_ok=True)

# Simpan model
model.export(saved_model_dir)
print("✅ Model disimpan dalam format SavedModel di:", saved_model_dir)

# Path
saved_model_dir = '/content/drive/MyDrive/Model CP/models/saved_model'
tflite_dir = '/content/drive/MyDrive/Model CP/models/tflite'
os.makedirs(tflite_dir, exist_ok=True)

# Konversi
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Simpan model TFLite
tflite_path = os.path.join(tflite_dir, 'model.tflite')
with open(tflite_path, 'wb') as f:
    f.write(tflite_model)

# Salin label file
os.system(f'cp class_names.txt {os.path.join(tflite_dir, "label.txt")}')

print("✅ Model berhasil dikonversi ke format TFLite dan disimpan di:", tflite_dir)

# Path
saved_model_dir = '/content/drive/MyDrive/Model CP/models/saved_model'
tfjs_dir = '/content/drive/MyDrive/Model CP/models/tfjs_model'
os.makedirs(tfjs_dir, exist_ok=True)

# Konversi ke TFJS
os.system(f'tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model {saved_model_dir} {tfjs_dir}')

print("✅ Model berhasil dikonversi ke format TensorFlow.js dan disimpan di:", tfjs_dir)

"""## Inference"""

# Path ke model dan labels
tflite_path = "/content/drive/MyDrive/Model CP/models/tflite/model.tflite"
labels_path = "/content/drive/MyDrive/Model CP/models/class_names.txt"

def tflite_inference(img_path, tflite_model_path, labels_path):
    # Load model
    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
    interpreter.allocate_tensors()

    # Input/output detail
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Load & preprocessing gambar
    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0) / 255.0

    # Set dan jalankan inference
    interpreter.set_tensor(input_details[0]['index'], img_array)
    interpreter.invoke()

    # Output prediksi
    predictions = interpreter.get_tensor(output_details[0]['index'])
    pred_class_idx = np.argmax(predictions[0])
    confidence = predictions[0][pred_class_idx]

    # Load label
    with open(labels_path, 'r') as f:
        labels = [line.strip() for line in f.readlines() if line.strip() != '']

    # Validasi index prediksi
    if pred_class_idx >= len(labels):
        print(f"[Warning] Predicted index {pred_class_idx} di luar jumlah label ({len(labels)}).")
        return f"Unknown (index {pred_class_idx})", confidence, img

    return labels[pred_class_idx], confidence, img

# Upload gambar dari lokal
uploaded = files.upload()

# Lakukan inference untuk setiap gambar yang diupload
for filename in uploaded.keys():
    pred_class, confidence, img = tflite_inference(filename, tflite_path, labels_path)

    # Tampilkan hasil
    plt.figure()
    plt.imshow(img)
    plt.title(f"{filename}\nPredicted: {pred_class}\nConfidence: {confidence:.2f}")
    plt.axis('off')
    plt.show()

# Path ke model dan labels
tflite_path = "/content/drive/MyDrive/Model CP/models/tflite/model.tflite"
labels_path = "/content/drive/MyDrive/Model CP/models/class_names.txt"

def tflite_inference(img_path, tflite_model_path, labels_path):
    # Load model
    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
    interpreter.allocate_tensors()

    # Input/output detail
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Load & preprocessing gambar
    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0) / 255.0

    # Set dan jalankan inference
    interpreter.set_tensor(input_details[0]['index'], img_array)
    interpreter.invoke()

    # Output prediksi
    predictions = interpreter.get_tensor(output_details[0]['index'])
    pred_class_idx = np.argmax(predictions[0])
    confidence = predictions[0][pred_class_idx]

    # Load label
    with open(labels_path, 'r') as f:
        labels = [line.strip() for line in f.readlines() if line.strip() != '']

    # Validasi index prediksi
    if pred_class_idx >= len(labels):
        print(f"[Warning] Predicted index {pred_class_idx} di luar jumlah label ({len(labels)}).")
        return f"Unknown (index {pred_class_idx})", confidence, img

    return labels[pred_class_idx], confidence, img

# Upload gambar dari lokal
uploaded = files.upload()

# Lakukan inference untuk setiap gambar yang diupload
for filename in uploaded.keys():
    pred_class, confidence, img = tflite_inference(filename, tflite_path, labels_path)

    # Tampilkan hasil
    plt.figure()
    plt.imshow(img)
    plt.title(f"{filename}\nPredicted: {pred_class}\nConfidence: {confidence:.2f}")
    plt.axis('off')
    plt.show()

